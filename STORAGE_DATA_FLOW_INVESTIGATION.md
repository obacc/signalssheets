# INVESTIGACIÃ“N: FLUJO DE DATOS A GCS - POLYGON PIPELINE

**Fecha:** 2025-11-15
**Branch:** claude/investigate-storage-data-flow-01Fh9HU3CrvwkSjEd1tY3TYN
**Bucket Investigado:** `gs://ss-bucket-polygon-incremental/polygon/daily/`
**Investigador:** Claude Code

---

## ğŸ¯ OBJETIVO

Determinar cÃ³mo llega la informaciÃ³n a la ruta de Google Cloud Storage:
`https://storage.googleapis.com/ss-bucket-polygon-incremental/polygon/daily/`

---

## ğŸ” HALLAZGOS PRINCIPALES

### âŒ Hallazgo CrÃ­tico: CÃ³digo de IngestiÃ³n NO EstÃ¡ en Este Repositorio

**ConclusiÃ³n:** El cÃ³digo que escribe datos a GCS **NO existe en este repositorio**. Este repositorio (`signalssheets`) es una aplicaciÃ³n frontend React/TypeScript que **consume** datos ya procesados, pero no los genera.

### âœ… Lo Que SÃ EncontrÃ©

1. **DocumentaciÃ³n completa del pipeline** en `/auditoria/AUDITORIA_POLYGON.md`
2. **Scripts de auditorÃ­a** para analizar el pipeline existente
3. **Referencias al bucket** en archivos de auditorÃ­a
4. **Arquitectura documentada** del flujo completo de datos

### âŒ Lo Que NO EncontrÃ©

1. CÃ³digo Python/JavaScript que escriba a GCS
2. Cloud Functions en el repositorio
3. Scripts de ingestiÃ³n desde Polygon API
4. Llamadas a la API de Polygon (`polygon.io`)
5. ConfiguraciÃ³n de pipelines ETL

---

## ğŸ“Š ARQUITECTURA DEL PIPELINE (Documentada)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASO 1: FUENTE DE DATOS                                â”‚
â”‚  Polygon API (polygon.io)                               â”‚
â”‚  - Datos histÃ³ricos de mercado (OHLCV)                  â”‚
â”‚  - S&P 500 stocks                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ â“ PROCESO DESCONOCIDO
                     â”‚    (NO en este repo)
                     â”‚
                     â”‚ Posibilidades:
                     â”‚ â”œâ”€ Cloud Function scheduled
                     â”‚ â”œâ”€ Script en otro repositorio
                     â”‚ â”œâ”€ Pipeline Airflow/Composer
                     â”‚ â””â”€ Servicio externo
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASO 2: ALMACENAMIENTO EN GCS                          â”‚
â”‚  ğŸ“‚ gs://ss-bucket-polygon-incremental/polygon/daily/   â”‚
â”‚                                                          â”‚
â”‚  Estructura:                                            â”‚
â”‚    polygon/daily/date=2025-11-15/*.parquet              â”‚
â”‚                   date=2025-11-14/*.parquet              â”‚
â”‚                   date=2025-11-13/*.parquet              â”‚
â”‚                   ...                                   â”‚
â”‚                                                          â”‚
â”‚  Formato: Apache Parquet (columnar)                     â”‚
â”‚  Particionamiento: Hive-style por fecha                 â”‚
â”‚  Campos esperados: ticker, date, open, high, low,       â”‚
â”‚                    close, volume, timestamp             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ [DATA TRANSFER SERVICE]
                     â”‚ - Cron: daily 07:00 UTC
                     â”‚ - SA: service-{NUM}@gcp-sa-bigquerydatatransfer...
                     â”‚ - Config: Parquet â†’ BigQuery
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASO 3: STAGING EN BIGQUERY                            â”‚
â”‚  ğŸ“Š sunny-advantage-471523-b3.market_data               â”‚
â”‚      .stg_prices_polygon_raw                            â”‚
â”‚                                                          â”‚
â”‚  Particionado: Por DATE                                 â”‚
â”‚  Clustered: Por ticker                                  â”‚
â”‚  Expiration: 30 dÃ­as                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ [SCHEDULED QUERY]
                     â”‚ - Cron: daily 08:00 UTC
                     â”‚ - Query: CALL sp_merge_polygon_prices()
                     â”‚ - OperaciÃ³n: MERGE idempotente
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PASO 4: TABLA FINAL                                    â”‚
â”‚  ğŸ“Š sunny-advantage-471523-b3.market_data.Prices        â”‚
â”‚                                                          â”‚
â”‚  Fuentes mÃºltiples: source = 'polygon'                  â”‚
â”‚  Particionado: Por DATE                                 â”‚
â”‚  Clustered: Por ticker, source                          â”‚
â”‚  RetenciÃ³n: Ilimitada (histÃ³rico)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ ARCHIVOS ANALIZADOS

### Archivos de AuditorÃ­a (DocumentaciÃ³n)

| Archivo | LÃ­neas | Hallazgos |
|---------|--------|-----------|
| `auditoria/AUDITORIA_POLYGON.md` | 1,134 | Arquitectura completa del pipeline documentada |
| `auditoria/README.md` | 339 | Instrucciones de auditorÃ­a |
| `auditoria/scripts/00_COMANDOS_COMPLETOS.sh` | 100+ | Scripts para auditar GCS, BQ, IAM |
| `auditoria/scripts/07_analisis_gcs_vs_bq.py` | - | ComparaciÃ³n GCS vs BigQuery |

### BÃºsquedas Realizadas

```bash
# BÃºsqueda 1: Referencias al bucket
grep -r "ss-bucket-polygon-incremental" .
# Resultado: Solo en archivos de auditorÃ­a

# BÃºsqueda 2: CÃ³digo Python
find . -name "*.py"
# Resultado: Solo scripts de auditorÃ­a (lectura, no escritura)

# BÃºsqueda 3: Escritura a GCS
grep -r "upload\|write\|put\|storage.Client\|bucket" **/*.py
# Resultado: Solo lectura en scripts de auditorÃ­a

# BÃºsqueda 4: Polygon API
grep -ri "polygon.*API\|polygon.*fetch\|polygon.*download"
# Resultado: Solo menciones en documentaciÃ³n

# BÃºsqueda 5: Cloud Functions
find . -path "*/cloud-functions/*" -o -path "*/functions/*"
# Resultado: No encontrado
```

---

## ğŸ” EVIDENCIA DEL PIPELINE EXTERNO

### Cita de la DocumentaciÃ³n

De `auditoria/AUDITORIA_POLYGON.md:540-543`:

```markdown
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FUENTE: Polygon API                                        â”‚
â”‚  (externo - asumido como operacional)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Palabras clave:** "externo - asumido como operacional"

Esto confirma que:
1. La fuente de datos es externa al repositorio
2. El proceso de ingestiÃ³n ya existe y funciona
3. La documentaciÃ³n solo audita el pipeline, no lo implementa

### Data Transfer Service (ConfiguraciÃ³n Esperada)

De `auditoria/AUDITORIA_POLYGON.md:635-651`:

```bash
bq mk --transfer_config \
  --data_source=google_cloud_storage \
  --display_name="Polygon Daily Load" \
  --params='{
    "data_path_template":"gs://ss-bucket-polygon-incremental/polygon/daily/date={run_date}/*.parquet",
    "destination_table_name_template":"stg_prices_polygon_raw",
    "file_format":"PARQUET",
    "write_disposition":"WRITE_APPEND"
  }' \
  --schedule="every day 07:00"
```

**InterpretaciÃ³n:** El Data Transfer Service lee de GCS (no escribe). Esto confirma que otro proceso debe escribir primero a GCS.

---

## ğŸ¯ UBICACIONES PROBABLES DEL CÃ“DIGO FALTANTE

### OpciÃ³n 1: Cloud Function en GCP (MÃ¡s Probable)

**Evidencia:**
- Mencionada en la documentaciÃ³n de auditorÃ­a
- TÃ­pico para pipelines scheduled en GCP
- No requiere versionado en este repo

**CÃ³mo verificar:**
```bash
gcloud functions list --project=sunny-advantage-471523-b3
gcloud functions list --gen2 --project=sunny-advantage-471523-b3
gcloud functions describe FUNCTION_NAME --format=json
```

**CÃ³digo tÃ­pico esperado:**
```python
# main.py (Cloud Function)
import os
from polygon import RESTClient
from google.cloud import storage
import pandas as pd
from datetime import datetime

def ingest_polygon_daily(request):
    """Scheduled Cloud Function para ingestar datos de Polygon"""

    # ConfiguraciÃ³n
    polygon_api_key = os.environ['POLYGON_API_KEY']
    bucket_name = 'ss-bucket-polygon-incremental'

    # Cliente Polygon
    client = RESTClient(polygon_api_key)

    # Obtener datos del dÃ­a anterior
    date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

    # Fetch data para S&P 500 tickers
    tickers = get_sp500_tickers()  # Lista de tickers

    data = []
    for ticker in tickers:
        aggs = client.get_aggs(
            ticker=ticker,
            multiplier=1,
            timespan="day",
            from_=date,
            to=date
        )
        data.extend(aggs)

    # Convertir a DataFrame
    df = pd.DataFrame(data)

    # Escribir a GCS como Parquet
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)

    blob = bucket.blob(f'polygon/daily/date={date}/data.parquet')
    blob.upload_from_string(
        df.to_parquet(index=False),
        content_type='application/octet-stream'
    )

    return {'status': 'success', 'date': date, 'records': len(df)}
```

### OpciÃ³n 2: Repositorio Separado de Backend

**Nombres probables:**
- `polygon-ingestion`
- `market-data-pipeline`
- `data-engineering`
- `etl-pipelines`

**CÃ³mo buscar:**
- Revisar la organizaciÃ³n de GitHub
- Preguntar al equipo de data engineering
- Revisar bitbucket/gitlab si existen otros repos

### OpciÃ³n 3: Cloud Composer (Airflow)

**Evidencia:** Mencionado en `auditoria/AUDITORIA_POLYGON.md:597-608` como opciÃ³n

**CÃ³mo verificar:**
```bash
gcloud composer environments list --project=sunny-advantage-471523-b3
```

**DAG tÃ­pico esperado:**
```python
# polygon_daily_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def fetch_and_upload():
    # LÃ³gica de ingestiÃ³n
    pass

with DAG(
    'polygon_daily_ingestion',
    schedule_interval='0 6 * * *',  # 06:00 UTC daily
    start_date=datetime(2024, 1, 1),
    catchup=False
) as dag:

    ingest_task = PythonOperator(
        task_id='fetch_polygon_data',
        python_callable=fetch_and_upload
    )
```

### OpciÃ³n 4: Cloud Scheduler â†’ HTTP Endpoint

**CÃ³mo verificar:**
```bash
gcloud scheduler jobs list --project=sunny-advantage-471523-b3
```

**ConfiguraciÃ³n esperada:**
```bash
gcloud scheduler jobs describe polygon-daily-ingest \
  --project=sunny-advantage-471523-b3
```

---

## ğŸš€ PRÃ“XIMOS PASOS RECOMENDADOS

### Para Encontrar el CÃ³digo Faltante

1. **Ejecutar scripts de auditorÃ­a** (requiere credenciales GCP):
   ```bash
   cd auditoria/scripts
   export GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json
   ./00_COMANDOS_COMPLETOS.sh
   ```

   Esto generarÃ¡:
   - `artifacts/cloud_functions_gen1.json`
   - `artifacts/cloud_functions_gen2.json`
   - `artifacts/cloud_scheduler_jobs.json`

2. **Revisar Cloud Functions via CLI:**
   ```bash
   gcloud functions list --project=sunny-advantage-471523-b3 --format=json
   ```

3. **Revisar Cloud Scheduler:**
   ```bash
   gcloud scheduler jobs list --project=sunny-advantage-471523-b3
   ```

4. **Revisar logs de escritura a GCS:**
   ```bash
   gcloud logging read '
     resource.type="gcs_bucket"
     AND resource.labels.bucket_name="ss-bucket-polygon-incremental"
     AND protoPayload.methodName="storage.objects.create"
   ' --limit=50 --format=json
   ```

5. **Buscar en otros repositorios:**
   - Revisar GitHub/GitLab de la organizaciÃ³n
   - Buscar repos con "polygon", "market-data", "etl"

6. **Preguntar al equipo:**
   - Data Engineering team
   - DevOps/Platform team
   - Revisar documentaciÃ³n interna/Confluence

---

## ğŸ“š REFERENCIAS

### Archivos de Este Repositorio

- `auditoria/AUDITORIA_POLYGON.md` - DocumentaciÃ³n completa del pipeline
- `auditoria/README.md` - Instrucciones de auditorÃ­a
- `auditoria/scripts/00_COMANDOS_COMPLETOS.sh` - Scripts bash de auditorÃ­a
- `auditoria/scripts/07_analisis_gcs_vs_bq.py` - AnÃ¡lisis Python GCS vs BQ

### Recursos GCP Identificados

| Recurso | UbicaciÃ³n | Estado |
|---------|-----------|--------|
| GCS Bucket | `gs://ss-bucket-polygon-incremental` | âœ… Existe (documentado) |
| Dataset | `sunny-advantage-471523-b3.market_data` | âœ… Existe (documentado) |
| Tabla Staging | `market_data.stg_prices_polygon_raw` | âœ… Existe (documentado) |
| Tabla Final | `market_data.Prices` | âœ… Existe (documentado) |
| Stored Procedure | `market_data.sp_merge_polygon_prices` | âœ… Existe (documentado) |
| Data Transfer Config | `Polygon Daily Load` | âš ï¸ Probablemente existe |
| Cloud Function | `???` | â“ Desconocido - A investigar |
| Cloud Scheduler | `???` | â“ Desconocido - A investigar |

---

## ğŸ“ CONCLUSIONES

### Resumen Ejecutivo

1. **El cÃ³digo de ingestiÃ³n NO estÃ¡ en este repositorio**
2. **Este repo es frontend (React/TypeScript)** que consume datos ya procesados
3. **El pipeline estÃ¡ documentado pero no implementado aquÃ­**
4. **La ingestiÃ³n probablemente ocurre vÃ­a:**
   - Cloud Function scheduled (mÃ¡s probable)
   - Repositorio separado de backend
   - Cloud Composer/Airflow DAG
   - Servicio externo

### Flujo de Datos Confirmado

```
[Polygon API]
     â†“ (â“ Proceso externo - NO en este repo)
[GCS: polygon/daily/date=YYYY-MM-DD/*.parquet]
     â†“ (âœ… Data Transfer Service - Documentado)
[BigQuery Staging: stg_prices_polygon_raw]
     â†“ (âœ… Scheduled Query - Documentado)
[BigQuery Final: Prices where source='polygon']
```

### Acciones Requeridas

Para responder completamente "Â¿cÃ³mo llega la informaciÃ³n?", se requiere:

1. âœ… **COMPLETADO:** Analizar este repositorio
2. â³ **PENDIENTE:** Ejecutar scripts de auditorÃ­a con credenciales GCP
3. â³ **PENDIENTE:** Revisar Cloud Functions en GCP
4. â³ **PENDIENTE:** Revisar Cloud Scheduler en GCP
5. â³ **PENDIENTE:** Buscar repositorio de backend/data-engineering
6. â³ **PENDIENTE:** Revisar logs de GCS para ver quÃ© proceso escribe

---

**InvestigaciÃ³n completada:** 2025-11-15
**Investigador:** Claude Code
**Branch:** `claude/investigate-storage-data-flow-01Fh9HU3CrvwkSjEd1tY3TYN`
