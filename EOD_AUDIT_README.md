# ğŸ” AUDITORÃA EOD PIPELINE - INSTRUCCIONES

## ğŸ“‹ RESUMEN

Este toolkit te permite auditar el pipeline completo de datos EOD (End of Day) de SignalsSheets para determinar el horario Ã³ptimo del CRON trigger del Worker de Cloudflare.

**Objetivo:** Mapear el flujo completo desde la descarga de datos de Polygon hasta el refresh del Worker, identificando los tiempos exactos de cada etapa.

---

## ğŸš€ INICIO RÃPIDO

### Prerequisitos

âœ… Python 3.7+ instalado
âœ… Acceso al proyecto GCP `sunny-advantage-471523-b3`
âœ… Service Account con permisos de BigQuery y Storage

### Paso 1: Configurar Credenciales de GCP

Tienes dos opciones:

#### OpciÃ³n A: Usando el script helper

```bash
# Crea el archivo de credenciales
cat > /tmp/gcp-sa-key.json <<'EOF'
{
  "type": "service_account",
  "project_id": "sunny-advantage-471523-b3",
  "private_key_id": "TU_PRIVATE_KEY_ID",
  "private_key": "-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n",
  "client_email": "claudecode@sunny-advantage-471523-b3.iam.gserviceaccount.com",
  "client_id": "TU_CLIENT_ID",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs"
}
EOF

# Ejecuta el setup
./setup_gcp_credentials.sh /tmp/gcp-sa-key.json
```

#### OpciÃ³n B: Manual

```bash
# Exportar directamente la variable de entorno
export GOOGLE_APPLICATION_CREDENTIALS="/ruta/a/tu/credenciales.json"

# Instalar dependencias
pip3 install google-cloud-bigquery google-cloud-storage google-cloud-logging pytz
```

### Paso 2: Ejecutar la AuditorÃ­a

```bash
# Ejecutar el script de auditorÃ­a
python3 audit_eod_pipeline.py
```

**Salida esperada:**
- Archivo JSON: `eod_pipeline_audit_results.json` con todos los datos recolectados
- Output en consola con informaciÃ³n detallada de cada fase

### Paso 3: Generar el Reporte

```bash
# Genera el reporte Markdown
python3 generate_eod_report.py

# Ver el reporte
cat DATA_PIPELINE_AUDIT_REPORT.md
```

---

## ğŸ“Š Â¿QUÃ‰ HACE LA AUDITORÃA?

### FASE 1: BigQuery - Tiempos de ActualizaciÃ³n

Analiza:
- âœ… Ãšltima actualizaciÃ³n de `market_data.prices`
- âœ… Ãšltima actualizaciÃ³n de `analytics.v_api_free_signals`
- âœ… DistribuciÃ³n horaria de updates (Ãºltimos 7 dÃ­as)
- âœ… Identifica patrones de actualizaciÃ³n

**Queries ejecutadas:**
```sql
-- 1. AnÃ¡lisis de tabla prices
SELECT MAX(date), MAX(updated_at), COUNT(*), COUNT(DISTINCT ticker)
FROM `market_data.prices`
WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAYS)

-- 2. AnÃ¡lisis de vista de seÃ±ales
SELECT MAX(signal_date), COUNT(*), COUNT(DISTINCT ticker)
FROM `analytics.v_api_free_signals`

-- 3. DistribuciÃ³n horaria
SELECT EXTRACT(HOUR FROM updated_at), COUNT(*), MIN(updated_at), MAX(updated_at)
FROM `market_data.prices`
WHERE DATE(updated_at) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAYS)
GROUP BY EXTRACT(HOUR FROM updated_at)
```

### FASE 2: GCS - Archivos Recientes

Analiza:
- âœ… Ãšltimos archivos cargados en `gs://ss-bucket-polygon-incremental/polygon/daily/`
- âœ… Fechas disponibles
- âœ… TamaÃ±o de archivos
- âœ… Timestamps de actualizaciÃ³n

### FASE 3: Scheduled Queries (Manual)

Documenta cÃ³mo listar scheduled queries:
```bash
bq ls --transfer_config --project_id=sunny-advantage-471523-b3
```

### FASE 4: Gap Analysis

Calcula:
- âœ… Diferencia temporal entre `prices` y `v_api_free_signals`
- âœ… Identifica si hay desfase entre datos y seÃ±ales

---

## ğŸ“ ESTRUCTURA DE ARCHIVOS

```
signalssheets/
â”œâ”€â”€ EOD_AUDIT_README.md                 â† Este archivo
â”œâ”€â”€ audit_eod_pipeline.py               â† Script principal de auditorÃ­a
â”œâ”€â”€ generate_eod_report.py              â† Generador de reporte Markdown
â”œâ”€â”€ setup_gcp_credentials.sh            â† Helper para configurar credenciales
â”œâ”€â”€ eod_pipeline_audit_results.json     â† Resultados (generado)
â””â”€â”€ DATA_PIPELINE_AUDIT_REPORT.md       â† Reporte final (generado)
```

---

## ğŸ¯ INFORMACIÃ“N DEL CLOUDFLARE WORKER

Para completar la auditorÃ­a, necesitamos informaciÃ³n sobre el Worker `free-api`:

### Â¿QuÃ© necesitamos?

1. **CÃ³digo del Worker**
   - Archivo principal (index.ts, worker.ts, etc.)
   - ConfiguraciÃ³n wrangler.toml del worker
   - CÃ³digo que hace fetch a `analytics.v_api_free_signals`

2. **ConfiguraciÃ³n actual del CRON**
   - Â¿CuÃ¡l es el schedule actual? (cada 10 min segÃºn contexto)
   - Â¿QuÃ© TTL tiene el cache KV?

3. **UbicaciÃ³n del Repositorio**
   - Â¿EstÃ¡ en un repo separado?
   - Â¿CÃ³mo se despliega actualmente?

### Compartir esta informaciÃ³n

Puedes:
- Proporcionar acceso al repositorio del worker
- Copiar y pegar el cÃ³digo relevante
- Ejecutar `wrangler tail --name free-api` para ver logs

---

## ğŸ”§ TROUBLESHOOTING

### Error: "Failed to retrieve metadata"

**Problema:** No hay credenciales configuradas

**SoluciÃ³n:**
```bash
export GOOGLE_APPLICATION_CREDENTIALS="/ruta/a/credenciales.json"
python3 audit_eod_pipeline.py
```

### Error: "Permission denied"

**Problema:** El service account no tiene permisos suficientes

**Permisos necesarios:**
- `roles/bigquery.dataViewer` - Para leer tablas
- `roles/bigquery.jobUser` - Para ejecutar queries
- `roles/storage.objectViewer` - Para leer GCS

**SoluciÃ³n:**
```bash
# Verificar permisos
gcloud projects get-iam-policy sunny-advantage-471523-b3 \
  --flatten="bindings[].members" \
  --filter="bindings.members:claudecode@sunny-advantage-471523-b3.iam.gserviceaccount.com"
```

### Error: "Table not found"

**Problema:** Nombre de tabla incorrecto o dataset no existe

**SoluciÃ³n:**
```bash
# Listar datasets
bq ls sunny-advantage-471523-b3

# Listar tablas en market_data
bq ls sunny-advantage-471523-b3:market_data

# Listar tablas en analytics
bq ls sunny-advantage-471523-b3:analytics
```

### No se encontraron archivos en GCS

**Problema:** Bucket vacÃ­o o prefijo incorrecto

**SoluciÃ³n:**
```bash
# Listar buckets
gsutil ls -p sunny-advantage-471523-b3

# Listar contenido del bucket
gsutil ls gs://ss-bucket-polygon-incremental/

# Ver estructura
gsutil ls gs://ss-bucket-polygon-incremental/polygon/
```

---

## ğŸ“– EJEMPLO DE OUTPUT

### Console Output (audit_eod_pipeline.py)

```
================================================================================
ğŸš€ AUDITORÃA COMPLETA - PIPELINE EOD SIGNALSSHEETS
================================================================================
ğŸ“… Fecha de ejecuciÃ³n: 2025-11-17 08:00:00 UTC
ğŸ”§ Proyecto: sunny-advantage-471523-b3
================================================================================

================================================================================
ğŸ” FASE 1: BIGQUERY - ANÃLISIS DE TIEMPOS DE ACTUALIZACIÃ“N
================================================================================

ğŸ“Š 1.1 - Analizando tabla market_data.prices...
  âœ… Ãšltima fecha de precios: 2025-11-16
  âœ… Ãšltima actualizaciÃ³n: 2025-11-17 07:30:00 UTC | 2025-11-17 01:30:00 CT
  ğŸ“ˆ Registros (Ãºltimos 7 dÃ­as): 125,432
  ğŸ¯ Tickers Ãºnicos: 5,231

ğŸ¯ 1.2 - Analizando vista analytics.v_api_free_signals...
  âœ… Ãšltima fecha de seÃ±ales: 2025-11-16
  ğŸ“Š Total de seÃ±ales: 234
  ğŸ¯ Tickers Ãºnicos: 234

...
```

### Reporte Final (DATA_PIPELINE_AUDIT_REPORT.md)

```markdown
# REPORTE DE AUDITORÃA - PIPELINE EOD SIGNALSSHEETS

**Proyecto:** `sunny-advantage-471523-b3`
**Fecha de AuditorÃ­a:** 2025-11-17T08:00:00Z
**Auditor:** Claude Code

---

## 1. EXECUTIVE SUMMARY

### Estado Actual del Pipeline

- **Ãšltima fecha de precios:** 2025-11-16
- **Ãšltima actualizaciÃ³n de prices:** 2025-11-17 07:30:00 UTC | 01:30:00 CT
- **Registros (Ãºltimos 7 dÃ­as):** 125,432
- **Ãšltima fecha de seÃ±ales:** 2025-11-16
- **Total de seÃ±ales:** 234
- **Gap entre prices y signals:** 0 dÃ­as
  - âœ… SeÃ±ales y precios estÃ¡n sincronizados

...

## 6. RECOMENDACIÃ“N FINAL

### Nuevo CRON Schedule Recomendado:

```toml
[triggers]
crons = ["0 9 * * 1-5"]  # 09:00 UTC = 03:00 CT (lunes a viernes)
```

**JustificaciÃ³n:**
- Prices se actualizan alrededor de las 07:00 UTC
- Damos 2 horas de margen para que el procesamiento de seÃ±ales complete
- Worker ejecutarÃ¡ a las 09:00 UTC (03:00 CT)

...
```

---

## ğŸ’¡ PRÃ“XIMOS PASOS

Una vez completada la auditorÃ­a:

1. âœ… **Revisar el reporte** - `DATA_PIPELINE_AUDIT_REPORT.md`
2. âœ… **Validar los horarios** - Confirmar con logs reales
3. âœ… **Actualizar Worker** - Modificar wrangler.toml con nuevo cron
4. âœ… **Actualizar TTL** - Cambiar cache TTL a 24 horas
5. âœ… **Desplegar** - `wrangler deploy --name free-api`
6. âœ… **Monitorear** - Verificar logs post-deployment

---

## ğŸ†˜ Â¿NECESITAS AYUDA?

### Para ejecutar la auditorÃ­a:

```bash
# 1. Configura credenciales
export GOOGLE_APPLICATION_CREDENTIALS="/ruta/a/credenciales.json"

# 2. Ejecuta auditorÃ­a
python3 audit_eod_pipeline.py

# 3. Genera reporte
python3 generate_eod_report.py

# 4. Lee el reporte
cat DATA_PIPELINE_AUDIT_REPORT.md
```

### Â¿Falta informaciÃ³n del Worker?

Comparte:
- CÃ³digo del worker (Ã­ndex.ts o similar)
- ConfiguraciÃ³n wrangler.toml
- Output de `wrangler tail --name free-api`

---

## ğŸ“š DOCUMENTACIÃ“N RELACIONADA

- [AuditorÃ­a Polygon Pipeline](auditoria/AUDITORIA_POLYGON.md) - Pipeline de carga GCS â†’ BigQuery
- [Scripts de auditorÃ­a existentes](auditoria/scripts/) - Comandos GCS/BQ/IAM
- [Queries SQL](auditoria/sql/) - AnÃ¡lisis de datos

---

**Creado por:** Claude Code
**VersiÃ³n:** 1.0
**Ãšltima actualizaciÃ³n:** 2025-11-17
