# üîç AUDITOR√çA PIPELINE POLYGON ‚Üí BIGQUERY

Toolkit completo para auditar el pipeline de carga de datos Polygon desde GCS hasta BigQuery.

## üì¶ Contenido

```
auditoria/
‚îú‚îÄ‚îÄ README.md                     ‚Üê Instrucciones (este archivo)
‚îú‚îÄ‚îÄ AUDITORIA_POLYGON.md          ‚Üê Informe completo con an√°lisis y recomendaciones
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ 00_COMANDOS_COMPLETOS.sh  ‚Üê Script maestro (GCS, BQ, IAM, Cloud Functions)
‚îÇ   ‚îú‚îÄ‚îÄ 05_diagnostico_logs_cloud.sh ‚Üê Extracci√≥n de logs de errores
‚îÇ   ‚îî‚îÄ‚îÄ 07_analisis_gcs_vs_bq.py  ‚Üê Comparaci√≥n Python (GCS vs Staging vs Prices)
‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îú‚îÄ‚îÄ 01_row_counts_staging.sql
‚îÇ   ‚îú‚îÄ‚îÄ 02_row_counts_prices.sql
‚îÇ   ‚îú‚îÄ‚îÄ 03_diff_staging_vs_prices.sql
‚îÇ   ‚îú‚îÄ‚îÄ 04_diagnostico_fallos_bq_jobs.sql
‚îÇ   ‚îî‚îÄ‚îÄ 06_analisis_calidad_datos.sql
‚îî‚îÄ‚îÄ artifacts/                    ‚Üê Resultados (CSV/JSON) - se generan al ejecutar
```

---

## ‚ö° INICIO R√ÅPIDO

### 1. Configurar Credenciales

Guarda el JSON de service account en un archivo seguro:

```bash
# Crear archivo de credenciales
cat > /tmp/gcp-sa-key.json <<'EOF'
{
  "type": "service_account",
  "project_id": "sunny-advantage-471523-b3",
  "private_key_id": "45e8e24c...",
  "private_key": "-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n",
  "client_email": "claudecode@sunny-advantage-471523-b3.iam.gserviceaccount.com",
  ...
}
EOF

# Exportar variable de entorno
export GOOGLE_APPLICATION_CREDENTIALS="/tmp/gcp-sa-key.json"

# Autenticar gcloud
gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS
gcloud config set project sunny-advantage-471523-b3
```

### 2. Instalar Dependencias

```bash
# Python 3.7+ requerido
pip3 install google-cloud-storage google-cloud-bigquery

# Verificar instalaci√≥n de herramientas GCP
gcloud --version
bq --version
gsutil --version
```

### 3. Ejecutar Auditor√≠a Completa

```bash
cd auditoria/scripts

# Opci√≥n 1: Script bash completo (GCS, BQ, IAM)
./00_COMANDOS_COMPLETOS.sh

# Opci√≥n 2: Solo logs de errores
./05_diagnostico_logs_cloud.sh

# Opci√≥n 3: An√°lisis Python (comparaci√≥n detallada)
python3 07_analisis_gcs_vs_bq.py
```

### 4. Consultas SQL Individuales

```bash
cd auditoria

# Row counts en staging (√∫ltimos 30 d√≠as)
bq query --use_legacy_sql=false --format=csv < sql/01_row_counts_staging.sql > artifacts/staging_counts.csv

# Row counts en Prices
bq query --use_legacy_sql=false --format=csv < sql/02_row_counts_prices.sql > artifacts/prices_counts.csv

# Diferencias staging vs prices
bq query --use_legacy_sql=false --format=csv < sql/03_diff_staging_vs_prices.sql > artifacts/diff.csv

# Diagn√≥stico de fallos (BigQuery jobs)
bq query --use_legacy_sql=false --format=csv < sql/04_diagnostico_fallos_bq_jobs.sql > artifacts/bq_errors.csv

# An√°lisis de calidad (duplicados, NULLs, anomal√≠as)
bq query --use_legacy_sql=false --format=csv < sql/06_analisis_calidad_datos.sql > artifacts/quality.csv
```

---

## üìä RESULTADOS ESPERADOS

Despu√©s de ejecutar los scripts, encontrar√°s en `artifacts/`:

### Archivos Generados por Scripts Bash

| Archivo | Descripci√≥n |
|---------|-------------|
| `gcs_dates_available.txt` | Lista de todas las fechas en GCS |
| `gcs_inventory.csv` | Detalles por fecha (archivos, bytes, MB) |
| `gcs_date_gaps.txt` | Gaps temporales detectados |
| `bq_datasets.json` | Todos los datasets del proyecto |
| `bq_tables_market_data.json` | Tablas en market_data |
| `schema_staging.json` | Schema de stg_prices_polygon_raw |
| `schema_prices.json` | Schema de Prices |
| `table_info_staging.json` | Info completa de staging (particiones, etc) |
| `table_info_prices.json` | Info completa de Prices |
| `routines.json` | Lista de rutinas en market_data |
| `sp_merge_polygon_prices.sql` | C√≥digo del Stored Procedure |
| `scheduled_queries.json` | Configuraciones de Scheduled Queries |
| `scheduled_queries_runs.json` | Historial de ejecuciones |
| `cloud_scheduler_jobs.json` | Jobs de Cloud Scheduler |
| `cloud_functions_*.json` | Funciones en GCP |
| `iam_*.json` | Pol√≠ticas IAM (proyecto, dataset, bucket) |
| `service_accounts_summary.txt` | SAs relevantes identificadas |

### Archivos de Logs

| Archivo | Descripci√≥n |
|---------|-------------|
| `logs_cloud_functions_errors.json` | Errores de Cloud Functions |
| `logs_cloud_scheduler_errors.json` | Errores de Cloud Scheduler |
| `logs_dts_errors.json` | Errores de Data Transfer Service |
| `logs_polygon_all.json` | Todos los logs con keyword "polygon" |
| `logs_top_errors.json` | Top errores agrupados por mensaje |
| `logs_error_frequency.csv` | Frecuencia de errores por d√≠a |

### Archivos de An√°lisis Python

| Archivo | Descripci√≥n |
|---------|-------------|
| `diff_gcs_staging_prices.csv` | Comparaci√≥n fecha por fecha |
| `comparison_summary.json` | Resumen ejecutivo con gaps |

### Archivos de Consultas SQL

| Archivo | Descripci√≥n |
|---------|-------------|
| `staging_counts.csv` | Row counts por fecha en staging |
| `prices_counts.csv` | Row counts por fecha en Prices |
| `diff_staging_vs_prices.csv` | Diferencias y status por fecha |
| `bq_jobs_errors.csv` | Jobs fallidos (√∫ltimos 14 d√≠as) |
| `data_quality.csv` | Duplicados, NULLs, anomal√≠as |

---

## üéØ CASOS DE USO

### Caso 1: "¬øPor qu√© no hay datos de ayer en Prices?"

```bash
# 1. Verificar si est√° en GCS
gsutil ls gs://ss-bucket-polygon-incremental/polygon/daily/ | grep $(date -d "yesterday" +%Y-%m-%d)

# 2. Verificar si lleg√≥ a staging
bq query --use_legacy_sql=false "
SELECT date, COUNT(*) as rows
FROM \`sunny-advantage-471523-b3.market_data.stg_prices_polygon_raw\`
WHERE date = DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)
GROUP BY date
"

# 3. Ejecutar an√°lisis completo
python3 scripts/07_analisis_gcs_vs_bq.py
```

### Caso 2: "¬øHay errores recientes en el pipeline?"

```bash
# 1. Ver errores de BigQuery jobs
bq query --use_legacy_sql=false < sql/04_diagnostico_fallos_bq_jobs.sql

# 2. Ver logs de Cloud (√∫ltimos 14 d√≠as)
./scripts/05_diagnostico_logs_cloud.sh

# 3. Revisar top errores
cat artifacts/logs_top_errors.json | jq '.[] | select(.occurrences > 5)'
```

### Caso 3: "¬øEl SP est√° duplicando datos?"

```bash
# Ejecutar an√°lisis de calidad
bq query --use_legacy_sql=false < sql/06_analisis_calidad_datos.sql

# Ver resultados de duplicados
grep "duplicate_count" artifacts/data_quality.csv
```

### Caso 4: "¬øQu√© permisos tiene la service account?"

```bash
# Ejecutar auditor√≠a IAM
cd scripts
./00_COMANDOS_COMPLETOS.sh  # Solo ejecuta secci√≥n 7

# Ver resumen
cat ../artifacts/service_accounts_summary.txt

# Ver policy completa del proyecto
cat ../artifacts/iam_project_policy.json | jq '.bindings[] | select(.members[] | contains("bigquerydatatransfer"))'
```

---

## üîß TROUBLESHOOTING

### Error: "Permission denied"

```bash
# Verificar autenticaci√≥n
gcloud auth list

# Verificar permisos de la SA
gcloud projects get-iam-policy sunny-advantage-471523-b3 \
  --flatten="bindings[].members" \
  --filter="bindings.members:claudecode@sunny-advantage-471523-b3.iam.gserviceaccount.com"
```

**Permisos m√≠nimos necesarios:**
- `roles/bigquery.dataViewer` (para leer tablas)
- `roles/bigquery.jobUser` (para ejecutar queries)
- `roles/storage.objectViewer` (para leer GCS)
- `roles/logging.viewer` (para leer logs)

### Error: "Table not found"

```bash
# Listar tablas existentes
bq ls sunny-advantage-471523-b3:market_data

# Verificar nombre exacto (case-sensitive)
bq show sunny-advantage-471523-b3:market_data.stg_prices_polygon_raw
```

### Error: "Not found: Dataset sunny-advantage-471523-b3:region-us"

En `sql/04_diagnostico_fallos_bq_jobs.sql`, ajustar la regi√≥n:

```sql
-- Probar con:
FROM `sunny-advantage-471523-b3.region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
-- O sin regi√≥n:
FROM `sunny-advantage-471523-b3`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
```

### Script Python falla con ImportError

```bash
# Instalar librer√≠as
pip3 install --upgrade google-cloud-storage google-cloud-bigquery

# Verificar instalaci√≥n
python3 -c "from google.cloud import bigquery, storage; print('OK')"
```

---

## üí∞ ESTIMACI√ìN DE COSTOS

Todos los scripts son de **SOLO LECTURA** y tienen costos m√≠nimos:

| Operaci√≥n | Costo Estimado |
|-----------|----------------|
| Listar GCS (`gsutil ls`) | $0 (operaciones gratis) |
| Leer metadatos BQ (`bq show`) | $0 (metadatos gratis) |
| Queries SQL auditor√≠a | ~$0.05 (10 GB escaneados) |
| Leer Cloud Logging | $0 (dentro de l√≠mites gratuitos) |
| **TOTAL estimado** | **< $0.10** |

**Nota:** Si las tablas NO est√°n particionadas, las queries SQL pueden escanear toda la tabla (mayor costo). Las consultas incluyen filtros de fecha para minimizar esto.

---

## üîí SEGURIDAD

### ‚ö†Ô∏è NO COMMITEAR CREDENCIALES

```bash
# Asegurar que .gitignore incluye:
echo "*.json" >> .gitignore
echo "artifacts/" >> .gitignore
echo "/tmp/*" >> .gitignore
```

### ‚úÖ Buenas Pr√°cticas

1. **Usar Service Account con permisos m√≠nimos** (no Owner/Editor)
2. **Rotar credenciales regularmente** (cada 90 d√≠as)
3. **No compartir `private_key`** en Slack/email
4. **Ejecutar desde entorno seguro** (VM con IAM, no laptop personal)
5. **Borrar `artifacts/` despu√©s de analizar** (pueden contener datos sensibles)

---

## üìö DOCUMENTACI√ìN COMPLETA

Lee el informe completo: **[AUDITORIA_POLYGON.md](AUDITORIA_POLYGON.md)**

Incluye:
- An√°lisis detallado de arquitectura
- Recomendaciones de configuraci√≥n
- Templates de Stored Procedures idempotentes
- Runbook operativo (troubleshooting, rollback)
- Checklist de implementaci√≥n To-Be
- Decisi√≥n justificada: Scheduled Query vs Dataform vs Composer

---

## üÜò SOPORTE

**Preguntas frecuentes:**
1. "¬øC√≥mo ejecuto solo una secci√≥n del script bash?"
   - Edita `00_COMANDOS_COMPLETOS.sh` y comenta las funciones que no necesitas

2. "¬øPuedo ejecutar esto en producci√≥n?"
   - S√≠, todos los scripts son read-only y seguros

3. "¬øQu√© hago con los resultados?"
   - Analiza los CSV/JSON para identificar gaps y errores
   - Lee el informe completo para recomendaciones
   - Implementa las correcciones sugeridas

**Autor:** Claude Code
**√öltima actualizaci√≥n:** 2025-11-11
**Versi√≥n:** 1.0
